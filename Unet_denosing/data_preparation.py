"""
æ•°æ®å‡†å¤‡æ¨¡å—ï¼šåŠ è½½ã€æ··åˆã€åˆ’åˆ† 10 ç»„ EEG æ•°æ®
æ”¯æŒ CSV æ ¼å¼æ•°æ®çš„ STFT é¢„å¤„ç†
"""

import numpy as np
import pandas as pd
import os
from pathlib import Path
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, List
import warnings
warnings.filterwarnings('ignore')


class EEGDataPreparation:
    """
    EEG æ•°æ®å‡†å¤‡ç±»ï¼šè´Ÿè´£åŠ è½½ã€æ··åˆå’Œåˆ’åˆ†æ•°æ®
    """
    def __init__(
        self,
        data_dir: str = "2_Data_processed",
        clean_prefix: str = "clean_Cz_",
        noisy_prefix: str = "raw_Cz_",
        num_files: int = 10,
        test_ratio: float = 0.2,
        random_seed: int = 42
    ):
        """
        Args:
            data_dir: æ•°æ®ç›®å½•
            clean_prefix: å¹²å‡€ä¿¡å·å‰ç¼€
            noisy_prefix: å¸¦å™ªä¿¡å·å‰ç¼€
            num_files: æ–‡ä»¶ç»„æ•°ï¼ˆé»˜è®¤ 10 ç»„ï¼‰
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            random_seed: éšæœºç§å­
        """
        self.data_dir = Path(data_dir)
        self.clean_prefix = clean_prefix
        self.noisy_prefix = noisy_prefix
        self.num_files = num_files
        self.test_ratio = test_ratio
        self.random_seed = random_seed
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        
    def load_all_data(self) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """
        åŠ è½½å…¨éƒ¨ 10 ç»„æ•°æ®
        
        Returns:
            clean_signals: å¹²å‡€ä¿¡å·åˆ—è¡¨
            noisy_signals: å¸¦å™ªä¿¡å·åˆ—è¡¨
        """
        clean_signals = []
        noisy_signals = []
        
        print(f"ğŸ“‚ å¼€å§‹åŠ è½½ {self.num_files} ç»„æ•°æ®...")
        
        for i in range(1, self.num_files + 1):
            clean_file = self.data_dir / f"{self.clean_prefix}{i:02d}.csv"
            noisy_file = self.data_dir / f"{self.noisy_prefix}{i:02d}.csv"
            
            if not clean_file.exists():
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {clean_file}")
                continue
            if not noisy_file.exists():
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {noisy_file}")
                continue
            
            # åŠ è½½ CSVï¼ˆå•åˆ—æ•°æ®ï¼‰
            clean_data = pd.read_csv(clean_file, header=None).values.flatten()
            noisy_data = pd.read_csv(noisy_file, header=None).values.flatten()
            
            # æ£€æŸ¥é•¿åº¦ä¸€è‡´æ€§
            if len(clean_data) != len(noisy_data):
                print(f"âš ï¸  é•¿åº¦ä¸åŒ¹é…: {clean_file} ({len(clean_data)}) vs {noisy_file} ({len(noisy_data)})")
                min_len = min(len(clean_data), len(noisy_data))
                clean_data = clean_data[:min_len]
                noisy_data = noisy_data[:min_len]
            
            clean_signals.append(clean_data)
            noisy_signals.append(noisy_data)
            
            print(f"âœ… ç¬¬ {i:02d} ç»„: {len(clean_data)} æ ·æœ¬")
        
        print(f"\nâœ… æ€»å…±åŠ è½½ {len(clean_signals)} ç»„æ•°æ®")
        return clean_signals, noisy_signals
    
    def segment_signals(
        self,
        clean_signals: List[np.ndarray],
        noisy_signals: List[np.ndarray],
        segment_length: int = 2048,
        overlap_ratio: float = 0.5
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        å°†é•¿ä¿¡å·åˆ‡åˆ†ä¸ºå›ºå®šé•¿åº¦çš„ç‰‡æ®µï¼Œå¹¶è¿›è¡Œæ··åˆ
        
        Args:
            clean_signals: å¹²å‡€ä¿¡å·åˆ—è¡¨
            noisy_signals: å¸¦å™ªä¿¡å·åˆ—è¡¨
            segment_length: ç‰‡æ®µé•¿åº¦
            overlap_ratio: é‡å æ¯”ä¾‹ï¼ˆ0-1ï¼‰
            
        Returns:
            clean_segments: (N, segment_length)
            noisy_segments: (N, segment_length)
        """
        clean_all = []
        noisy_all = []
        
        hop_length = int(segment_length * (1 - overlap_ratio))
        
        print(f"\nğŸ”ª åˆ‡åˆ†å‚æ•°: segment_length={segment_length}, overlap={overlap_ratio}")
        
        for clean_sig, noisy_sig in zip(clean_signals, noisy_signals):
            num_segments = (len(clean_sig) - segment_length) // hop_length + 1
            
            for j in range(num_segments):
                start = j * hop_length
                end = start + segment_length
                
                if end > len(clean_sig):
                    break
                
                clean_all.append(clean_sig[start:end])
                noisy_all.append(noisy_sig[start:end])
        
        clean_segments = np.array(clean_all, dtype=np.float32)
        noisy_segments = np.array(noisy_all, dtype=np.float32)
        
        print(f"âœ… åˆ‡åˆ†å®Œæˆ: {len(clean_segments)} ä¸ªç‰‡æ®µ")
        
        # å®Œå…¨éšæœºæ··åˆ
        indices = np.arange(len(clean_segments))
        np.random.shuffle(indices)
        
        clean_segments = clean_segments[indices]
        noisy_segments = noisy_segments[indices]
        
        print(f"âœ… æ•°æ®å·²å®Œå…¨æ··åˆï¼ˆrandom_seed={self.random_seed}ï¼‰")
        
        return clean_segments, noisy_segments
    
    def split_train_test(
        self,
        clean_segments: np.ndarray,
        noisy_segments: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
        
        Returns:
            clean_train, clean_test, noisy_train, noisy_test
        """
        clean_train, clean_test, noisy_train, noisy_test = train_test_split(
            clean_segments,
            noisy_segments,
            test_size=self.test_ratio,
            random_state=self.random_seed
        )
        
        print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"   è®­ç»ƒé›†: {len(clean_train)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(clean_test)} æ ·æœ¬")
        print(f"   æ¯”ä¾‹: {1-self.test_ratio:.0%} / {self.test_ratio:.0%}")
        
        return clean_train, clean_test, noisy_train, noisy_test
    
    def save_dataset(
        self,
        clean_train: np.ndarray,
        clean_test: np.ndarray,
        noisy_train: np.ndarray,
        noisy_test: np.ndarray,
        output_dir: str = "dataset_mixed"
    ):
        """
        ä¿å­˜åˆ’åˆ†å¥½çš„æ•°æ®é›†
        """
        output_path = Path(output_dir)
        
        # åˆ›å»ºç›®å½•ç»“æ„
        for split in ['train', 'test']:
            for category in ['clean', 'noisy']:
                (output_path / split / category).mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸º .npy
        np.save(output_path / 'train' / 'clean' / 'data.npy', clean_train)
        np.save(output_path / 'train' / 'noisy' / 'data.npy', noisy_train)
        np.save(output_path / 'test' / 'clean' / 'data.npy', clean_test)
        np.save(output_path / 'test' / 'noisy' / 'data.npy', noisy_test)
        
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
        print(f"   train/clean/data.npy: {clean_train.shape}")
        print(f"   train/noisy/data.npy: {noisy_train.shape}")
        print(f"   test/clean/data.npy: {clean_test.shape}")
        print(f"   test/noisy/data.npy: {noisy_test.shape}")


class STFTDataset(Dataset):
    """
    STFT æ•°æ®é›†ï¼šå®æ—¶è®¡ç®— STFT å¹¶è¿”å›é¢‘è°±
    """
    def __init__(
        self,
        clean_data: np.ndarray,
        noisy_data: np.ndarray,
        n_fft: int = 256,
        hop_length: int = 64,
        mode: str = 'magnitude',  # 'magnitude' æˆ– 'complex'
        normalize: bool = True
    ):
        """
        Args:
            clean_data: å¹²å‡€ä¿¡å· (N, signal_length)
            noisy_data: å¸¦å™ªä¿¡å· (N, signal_length)
            n_fft: FFT ç‚¹æ•°
            hop_length: å¸§ç§»
            mode: 'magnitude' (ä»…å¹…åº¦è°±) æˆ– 'complex' (å®éƒ¨+è™šéƒ¨)
            normalize: æ˜¯å¦å½’ä¸€åŒ–
        """
        self.clean_data = clean_data
        self.noisy_data = noisy_data
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.mode = mode
        self.normalize = normalize
        
        # é¢„è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰
        if normalize:
            sample_stft = self._compute_stft(noisy_data[0])
            self.mean = 0.0
            self.std = 1.0  # å°†åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è®¡ç®—
    
    def _compute_stft(self, signal: np.ndarray) -> np.ndarray:
        """
        è®¡ç®— STFT
        
        Returns:
            å¦‚æœ mode='magnitude': (freq, time)
            å¦‚æœ mode='complex': (2, freq, time) [Re, Im]
        """
        # ä½¿ç”¨ numpy FFT
        stft = np.array([
            np.fft.rfft(signal[i:i+self.n_fft])
            for i in range(0, len(signal) - self.n_fft + 1, self.hop_length)
        ]).T  # (freq, time)
        
        if self.mode == 'magnitude':
            return np.abs(stft)
        elif self.mode == 'complex':
            real = np.real(stft)
            imag = np.imag(stft)
            return np.stack([real, imag], axis=0)  # (2, freq, time)
        else:
            raise ValueError(f"Unsupported mode: {self.mode}")
    
    def __len__(self):
        return len(self.clean_data)
    
    def __getitem__(self, idx):
        clean_sig = self.clean_data[idx]
        noisy_sig = self.noisy_data[idx]
        
        # è®¡ç®— STFT
        clean_stft = self._compute_stft(clean_sig)
        noisy_stft = self._compute_stft(noisy_sig)
        
        # å½’ä¸€åŒ–ï¼ˆç®€å•çš„ min-max æˆ– z-scoreï¼‰
        if self.normalize:
            # ä½¿ç”¨ log-scale å½’ä¸€åŒ–ï¼ˆå¸¸ç”¨äºéŸ³é¢‘/EEGï¼‰
            if self.mode == 'magnitude':
                noisy_stft = np.log1p(noisy_stft)  # log(1 + x)
                clean_stft = np.log1p(clean_stft)
                
                # Z-score å½’ä¸€åŒ–
                noisy_stft = (noisy_stft - noisy_stft.mean()) / (noisy_stft.std() + 1e-8)
                clean_stft = (clean_stft - clean_stft.mean()) / (clean_stft.std() + 1e-8)
            else:
                # å¯¹å®éƒ¨è™šéƒ¨åˆ†åˆ«å½’ä¸€åŒ–
                for c in range(2):
                    noisy_stft[c] = (noisy_stft[c] - noisy_stft[c].mean()) / (noisy_stft[c].std() + 1e-8)
                    clean_stft[c] = (clean_stft[c] - clean_stft[c].mean()) / (clean_stft[c].std() + 1e-8)
        
        # è½¬ä¸º Tensor å¹¶æ·»åŠ  channel ç»´åº¦
        if self.mode == 'magnitude':
            noisy_stft = torch.FloatTensor(noisy_stft).unsqueeze(0)  # (1, freq, time)
            clean_stft = torch.FloatTensor(clean_stft).unsqueeze(0)
        else:
            noisy_stft = torch.FloatTensor(noisy_stft)  # (2, freq, time)
            clean_stft = torch.FloatTensor(clean_stft)
        
        return noisy_stft, clean_stft


def create_dataloaders(
    clean_train: np.ndarray,
    clean_test: np.ndarray,
    noisy_train: np.ndarray,
    noisy_test: np.ndarray,
    batch_size: int = 16,
    n_fft: int = 256,
    hop_length: int = 64,
    mode: str = 'magnitude',
    num_workers: int = 0
) -> Tuple[DataLoader, DataLoader]:
    """
    åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯• DataLoader
    """
    train_dataset = STFTDataset(
        clean_train, noisy_train,
        n_fft=n_fft, hop_length=hop_length, mode=mode, normalize=True
    )
    
    test_dataset = STFTDataset(
        clean_test, noisy_test,
        n_fft=n_fft, hop_length=hop_length, mode=mode, normalize=True
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    print(f"\nğŸ”„ DataLoader å·²åˆ›å»º:")
    print(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)} (batch_size={batch_size})")
    print(f"   æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    return train_loader, test_loader


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®å‡†å¤‡æµç¨‹
    prep = EEGDataPreparation(
        data_dir="2_Data_processed",
        num_files=10,
        test_ratio=0.2,
        random_seed=42
    )
    
    # 1. åŠ è½½æ•°æ®
    clean_signals, noisy_signals = prep.load_all_data()
    
    # 2. åˆ‡åˆ†ä¸æ··åˆ
    clean_segments, noisy_segments = prep.segment_signals(
        clean_signals, noisy_signals,
        segment_length=2048,
        overlap_ratio=0.5
    )
    
    # 3. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    clean_train, clean_test, noisy_train, noisy_test = prep.split_train_test(
        clean_segments, noisy_segments
    )
    
    # 4. ä¿å­˜
    prep.save_dataset(clean_train, clean_test, noisy_train, noisy_test)
    
    # 5. åˆ›å»º DataLoader
    train_loader, test_loader = create_dataloaders(
        clean_train, clean_test, noisy_train, noisy_test,
        batch_size=16,
        n_fft=256,
        hop_length=64,
        mode='magnitude'
    )
    
    # æµ‹è¯•ä¸€ä¸ª batch
    noisy_batch, clean_batch = next(iter(train_loader))
    print(f"\nğŸ“¦ Batch å½¢çŠ¶:")
    print(f"   Noisy: {noisy_batch.shape}")
    print(f"   Clean: {clean_batch.shape}")
