"""
U-Net Denoising AutoEncoder è®­ç»ƒè„šæœ¬
æ”¯æŒæ–­ç‚¹ç»­è®­ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœç­‰åŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import os
from pathlib import Path
from tqdm import tqdm
import argparse
import json
from datetime import datetime

from data_preparation import EEGDataPreparation, create_dataloaders
from unet_model import build_unet, CombinedLoss


class Trainer:
    """
    U-Net è®­ç»ƒå™¨
    """
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        test_loader: DataLoader,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        scheduler: optim.lr_scheduler._LRScheduler = None,
        device: str = 'cuda',
        checkpoint_dir: str = 'checkpoints_unet',
        log_dir: str = 'logs_unet'
    ):
        self.model = model
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        
        # åˆ›å»ºç›®å½•
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
    
    def train_epoch(self):
        """
        è®­ç»ƒä¸€ä¸ª epoch
        """
        self.model.train()
        epoch_loss = 0.0
        epoch_l1 = 0.0
        epoch_l2 = 0.0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch}")
        
        for batch_idx, (noisy, clean) in enumerate(pbar):
            noisy = noisy.to(self.device)
            clean = clean.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            pred = self.model(noisy)
            
            # è®¡ç®—æŸå¤±
            loss, loss_dict = self.criterion(pred, clean)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            epoch_loss += loss_dict['total']
            epoch_l1 += loss_dict['l1']
            epoch_l2 += loss_dict['l2']
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss_dict['total']:.4f}",
                'l1': f"{loss_dict['l1']:.4f}",
                'l2': f"{loss_dict['l2']:.4f}"
            })
            
            # TensorBoard è®°å½•
            global_step = self.current_epoch * len(self.train_loader) + batch_idx
            self.writer.add_scalar('Train/Loss', loss_dict['total'], global_step)
            self.writer.add_scalar('Train/L1', loss_dict['l1'], global_step)
            self.writer.add_scalar('Train/L2', loss_dict['l2'], global_step)
        
        # å¹³å‡æŸå¤±
        avg_loss = epoch_loss / len(self.train_loader)
        avg_l1 = epoch_l1 / len(self.train_loader)
        avg_l2 = epoch_l2 / len(self.train_loader)
        
        return avg_loss, avg_l1, avg_l2
    
    @torch.no_grad()
    def validate(self):
        """
        éªŒè¯
        """
        self.model.eval()
        epoch_loss = 0.0
        epoch_l1 = 0.0
        epoch_l2 = 0.0
        
        for noisy, clean in tqdm(self.test_loader, desc="Validation"):
            noisy = noisy.to(self.device)
            clean = clean.to(self.device)
            
            # å‰å‘ä¼ æ’­
            pred = self.model(noisy)
            
            # è®¡ç®—æŸå¤±
            loss, loss_dict = self.criterion(pred, clean)
            
            epoch_loss += loss_dict['total']
            epoch_l1 += loss_dict['l1']
            epoch_l2 += loss_dict['l2']
        
        avg_loss = epoch_loss / len(self.test_loader)
        avg_l1 = epoch_l1 / len(self.test_loader)
        avg_l2 = epoch_l2 / len(self.test_loader)
        
        return avg_loss, avg_l1, avg_l2
    
    def save_checkpoint(self, is_best: bool = False):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        """
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': self.best_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        latest_path = self.checkpoint_dir / 'latest.pth'
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.checkpoint_dir / 'best.pth'
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
        
        # å®šæœŸä¿å­˜ epoch æ¨¡å‹
        if self.current_epoch % 10 == 0:
            epoch_path = self.checkpoint_dir / f'epoch_{self.current_epoch}.pth'
            torch.save(checkpoint, epoch_path)
    
    def load_checkpoint(self, checkpoint_path: str):
        """
        åŠ è½½æ£€æŸ¥ç‚¹
        """
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.current_epoch = checkpoint['epoch']
        self.best_loss = checkpoint['best_loss']
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        
        if self.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        print(f"âœ… æ£€æŸ¥ç‚¹å·²åŠ è½½: epoch {self.current_epoch}, best_loss {self.best_loss:.4f}")
    
    def train(self, num_epochs: int, early_stop_patience: int = 10):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        """
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ U-Net Denoising AutoEncoder")
        print(f"   æ€» Epoch: {num_epochs}")
        print(f"   æ—©åœè€å¿ƒ: {early_stop_patience}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']}")
        
        patience_counter = 0
        
        for epoch in range(self.current_epoch + 1, num_epochs + 1):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_loss, train_l1, train_l2 = self.train_epoch()
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss, val_l1, val_l2 = self.validate()
            self.val_losses.append(val_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # TensorBoard è®°å½•
            self.writer.add_scalar('Epoch/Train_Loss', train_loss, epoch)
            self.writer.add_scalar('Epoch/Val_Loss', val_loss, epoch)
            self.writer.add_scalar('Epoch/Learning_Rate', self.optimizer.param_groups[0]['lr'], epoch)
            
            # æ‰“å°ä¿¡æ¯
            print(f"\nğŸ“Š Epoch {epoch}/{num_epochs}")
            print(f"   Train Loss: {train_loss:.4f} (L1: {train_l1:.4f}, L2: {train_l2:.4f})")
            print(f"   Val Loss:   {val_loss:.4f} (L1: {val_l1:.4f}, L2: {val_l2:.4f})")
            print(f"   LR: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_loss
            if is_best:
                self.best_loss = val_loss
                patience_counter = 0
                print(f"   â­ æ–°æœ€ä½³æ¨¡å‹! Val Loss: {val_loss:.4f}")
            else:
                patience_counter += 1
            
            self.save_checkpoint(is_best)
            
            # æ—©åœ
            if patience_counter >= early_stop_patience:
                print(f"\nâ¹ï¸  æ—©åœè§¦å‘! {early_stop_patience} ä¸ª epoch æ— æ”¹å–„")
                break
        
        self.writer.close()
        print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³ Val Loss: {self.best_loss:.4f}")


def main(args):
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    # ====== 1. æ•°æ®å‡†å¤‡ ======
    print("\n" + "="*50)
    print("1ï¸âƒ£  æ•°æ®å‡†å¤‡")
    print("="*50)
    
    if args.use_prepared_data and os.path.exists('dataset_mixed/train/clean/data.npy'):
        # åŠ è½½å·²å‡†å¤‡å¥½çš„æ•°æ®
        print("ğŸ“‚ åŠ è½½å·²å‡†å¤‡å¥½çš„æ•°æ®...")
        clean_train = np.load('dataset_mixed/train/clean/data.npy')
        clean_test = np.load('dataset_mixed/test/clean/data.npy')
        noisy_train = np.load('dataset_mixed/train/noisy/data.npy')
        noisy_test = np.load('dataset_mixed/test/noisy/data.npy')
    else:
        # é‡æ–°å‡†å¤‡æ•°æ®
        prep = EEGDataPreparation(
            data_dir=args.data_dir,
            num_files=args.num_files,
            test_ratio=args.test_ratio,
            random_seed=args.seed
        )
        
        clean_signals, noisy_signals = prep.load_all_data()
        clean_segments, noisy_segments = prep.segment_signals(
            clean_signals, noisy_signals,
            segment_length=args.segment_length,
            overlap_ratio=args.overlap_ratio
        )
        clean_train, clean_test, noisy_train, noisy_test = prep.split_train_test(
            clean_segments, noisy_segments
        )
        prep.save_dataset(clean_train, clean_test, noisy_train, noisy_test)
    
    # åˆ›å»º DataLoader
    train_loader, test_loader = create_dataloaders(
        clean_train, clean_test, noisy_train, noisy_test,
        batch_size=args.batch_size,
        n_fft=args.n_fft,
        hop_length=args.hop_length,
        mode=args.mode,
        num_workers=args.num_workers
    )
    
    # ====== 2. æ„å»ºæ¨¡å‹ ======
    print("\n" + "="*50)
    print("2ï¸âƒ£  æ„å»ºæ¨¡å‹")
    print("="*50)
    
    model = build_unet(
        mode=args.mode,
        base_channels=args.base_channels,
        depth=args.depth,
        device=device
    )
    
    # ====== 3. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ======
    print("\n" + "="*50)
    print("3ï¸âƒ£  æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨")
    print("="*50)
    
    criterion = CombinedLoss(
        l1_weight=args.l1_weight,
        l2_weight=args.l2_weight,
        use_multiscale=args.use_multiscale
    )
    
    optimizer = optim.Adam(
        model.parameters(),
        lr=args.lr,
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if args.scheduler == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    elif args.scheduler == 'plateau':
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5
        )
    elif args.scheduler == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    else:
        scheduler = None
    
    print(f"   æŸå¤±å‡½æ•°: L1({args.l1_weight}) + L2({args.l2_weight})")
    print(f"   ä¼˜åŒ–å™¨: Adam (lr={args.lr}, weight_decay={args.weight_decay})")
    print(f"   è°ƒåº¦å™¨: {args.scheduler}")
    
    # ====== 4. è®­ç»ƒå™¨ ======
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        test_loader=test_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        checkpoint_dir=args.checkpoint_dir,
        log_dir=args.log_dir
    )
    
    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if args.resume:
        checkpoint_path = Path(args.checkpoint_dir) / 'latest.pth'
        if checkpoint_path.exists():
            trainer.load_checkpoint(str(checkpoint_path))
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # ====== 5. è®­ç»ƒ ======
    print("\n" + "="*50)
    print("4ï¸âƒ£  å¼€å§‹è®­ç»ƒ")
    print("="*50)
    
    trainer.train(num_epochs=args.epochs, early_stop_patience=args.patience)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config = vars(args)
    config['best_loss'] = trainer.best_loss
    config['trained_epochs'] = trainer.current_epoch
    
    config_path = Path(args.checkpoint_dir) / 'config.json'
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    
    print(f"\nğŸ’¾ è®­ç»ƒé…ç½®å·²ä¿å­˜: {config_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='U-Net Denoising AutoEncoder Training')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_dir', type=str, default='2_Data_processed', help='æ•°æ®ç›®å½•')
    parser.add_argument('--num_files', type=int, default=10, help='æ•°æ®æ–‡ä»¶æ•°é‡')
    parser.add_argument('--segment_length', type=int, default=2048, help='ä¿¡å·ç‰‡æ®µé•¿åº¦')
    parser.add_argument('--overlap_ratio', type=float, default=0.5, help='ç‰‡æ®µé‡å æ¯”ä¾‹')
    parser.add_argument('--test_ratio', type=float, default=0.2, help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--use_prepared_data', action='store_true', help='ä½¿ç”¨å·²å‡†å¤‡å¥½çš„æ•°æ®')
    
    # STFT å‚æ•°
    parser.add_argument('--n_fft', type=int, default=256, help='FFT ç‚¹æ•°')
    parser.add_argument('--hop_length', type=int, default=64, help='å¸§ç§»')
    parser.add_argument('--mode', type=str, default='magnitude', choices=['magnitude', 'complex'], 
                        help='STFT æ¨¡å¼')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base_channels', type=int, default=64, help='åŸºç¡€é€šé“æ•°')
    parser.add_argument('--depth', type=int, default=4, help='ç½‘ç»œæ·±åº¦')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=16, help='Batch size')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒ epoch æ•°')
    parser.add_argument('--lr', type=float, default=1e-3, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='æƒé‡è¡°å‡')
    parser.add_argument('--scheduler', type=str, default='plateau', 
                        choices=['step', 'plateau', 'cosine', 'none'], help='å­¦ä¹ ç‡è°ƒåº¦å™¨')
    parser.add_argument('--patience', type=int, default=15, help='æ—©åœè€å¿ƒ')
    
    # æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument('--l1_weight', type=float, default=1.0, help='L1 æŸå¤±æƒé‡')
    parser.add_argument('--l2_weight', type=float, default=0.5, help='L2 æŸå¤±æƒé‡')
    parser.add_argument('--use_multiscale', action='store_true', help='ä½¿ç”¨å¤šå°ºåº¦æŸå¤±')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--num_workers', type=int, default=0, help='DataLoader workers')
    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints_unet', help='æ£€æŸ¥ç‚¹ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='logs_unet', help='æ—¥å¿—ç›®å½•')
    parser.add_argument('--resume', action='store_true', help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    main(args)
